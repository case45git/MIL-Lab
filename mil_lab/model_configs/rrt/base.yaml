mlp_dim: 512
embed_dim: 512
act: "relu"
dropout: 0.25
pos_pos: 0
pos: "ppeg"
peg_k: 7
attn: "ntrans"
pool: "attn"
region_num: 8
n_layers: 2
n_heads: 8
multi_scale: false
drop_path: 0.0
da_act: "relu"
trans_dropout: 0.1
ffn: false
ffn_act: "gelu"
mlp_ratio: 4.0
da_gated: false
da_bias: false
da_dropout: false
trans_dim: 64
n_cycle: 1
epeg: false
min_region_num: 0
qkv_bias: true
shift_size: false
no_norm: false